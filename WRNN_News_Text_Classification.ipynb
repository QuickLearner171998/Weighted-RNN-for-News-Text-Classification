{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WRNN-News_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ql_Q4mRiqBMk",
        "JQkufQxFqF1R",
        "EDQCVLzrhobu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/Weighted-RNN-for-News-Text-Classification/blob/master/WRNN_News_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F5vE8urN7n9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CmxHdbUDQpU",
        "colab_type": "code",
        "outputId": "b6904cd0-b614-40bd-e9ba-ff69f5a5b459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd \"/content/drive/My Drive/nnfl_project/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/nnfl_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V6VsSotwNhy",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8zYf1TVwM5J",
        "colab_type": "code",
        "outputId": "d19d0331-82f7-4f0f-b6cb-6c9828474303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras.layers import *\n",
        "#Input,Activation, Dense, Embedding, Flatten, LSTM, Conv1D,MaxPooling1D,Dropout,Bidirectional,Reshape,MaxPooling1D,GlobalMaxPooling1D, Dropout,Permute,TimeDistributed,BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential, Model\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.models import load_model\n",
        "from keras import regularizers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "\n",
        "PATH = '/content/drive/My Drive/nnfl_project/'\n",
        "DATA_PATH = '/content/drive/My Drive/nnfl_project/data'\n",
        "GLOVE_PATH = '/content/drive/My Drive/glove.840B.300d.txt'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8x0nRb9BJUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(model,y_texts, y_labels, batch_size):\n",
        "  loss, acc = model.evaluate(y_texts,y_labels,batch_size=batch_size,verbose=1)\n",
        "  # print(\"Accuracy of the model\",acc)\n",
        "  y_pred = model.predict(y_texts, batch_size=batch_size)\n",
        "  y_pred_max_ind = np.argmax(y_pred,axis=1)\n",
        "  y_true_max_ind = np.argmax(y_labels,axis=1)\n",
        "  cf = (classification_report(y_true_max_ind,y_pred_max_ind))\n",
        "\n",
        "  print(\"\\n Model Accuracy : \",acc)\n",
        "  print(\"\\nClassification Report\\n \",cf)\n",
        "\n",
        "\n",
        "\n",
        "# model_op_folder = 'new_conv1d_GMP_split'\n",
        "def get_callbacks():\n",
        "\n",
        "  chk_pth = (\"{epoch:02d}_{val_categorical_accuracy:.2f}.h5\")\n",
        "  # early stopping\n",
        "  earlyStop = EarlyStopping(monitor='val_loss', mode='auto', verbose=1,patience=16)\n",
        "  checkpoint = ModelCheckpoint(chk_pth,monitor='val_loss', verbose=1, save_weights_only=True, save_best_only='True',mode = 'auto', period=2)\n",
        "\n",
        "  # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001, verbose = 1)\n",
        "  cb = [checkpoint,earlyStop]\n",
        "  return cb\n",
        "\n",
        "\n",
        "\n",
        "def run_model(model,train_tokenized_seq,train_labels,epochs,batch_size,vsplit,init_epoch=0):\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01),metrics=['categorical_accuracy'])\n",
        "\n",
        "  # check if a saved_model exists\n",
        "  if(glob.glob('*.h5')):\n",
        "    print(\"loading model\")\n",
        "    load_path = return_last_saved_model(glob.glob('*.h5'))\n",
        "    # model = load_model(load_path)\n",
        "    model.load_weights(load_path)\n",
        "    init_epoch = get_epoch_frm_model_file(load_path)\n",
        "\n",
        "\n",
        "  print(\"START TRAINING\")\n",
        "\n",
        "  # history = model.fit(train_tokenized_seq,train_labels,epochs = init_epoch+epochs, batch_size=batch_size, validation_data=(test_tokenized_seq, test_labels), callbacks=[checkpoint,earlyStop], initial_epoch=init_epoch)\n",
        "  history = model.fit(train_tokenized_seq,train_labels,epochs = init_epoch+epochs, batch_size=batch_size, validation_split=vsplit, callbacks=get_callbacks(), initial_epoch=init_epoch)\n",
        "\n",
        "\n",
        "  print(\"Training fininshed\")\n",
        "  # save_path = \"{}_epochs-model.h5\".format(init_epoch+epochs)\n",
        "  # model.save(save_path)\n",
        "  return history\n",
        "\n",
        "\n",
        "\n",
        "# plot model\n",
        "\n",
        "def training_plots(history, model_name,model):  \n",
        "  plt.plot(history.history[\"categorical_accuracy\"])\n",
        "  plt.plot(history.history[\"val_categorical_accuracy\"])\n",
        "  plt.title('model acc')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history[\"loss\"])\n",
        "  plt.plot(history.history[\"val_loss\"])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plot_model(model,to_file=model_name,show_shapes=True)\n",
        "\n",
        "\n",
        "def tokenize_sequences(texts, sl,tokenizer):\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  sequences = pad_sequences(sequences, maxlen=sl)\n",
        "\n",
        "  return sequences\n",
        "\n",
        "def get_data(data_path):\n",
        "  newsgroups_data_train = fetch_20newsgroups(data_path,remove=('headers', 'footers', 'quotes'),subset='train')\n",
        "  train_labels = newsgroups_data_train.target\n",
        "  train_texts = newsgroups_data_train.data\n",
        "\n",
        "  newsgroups_data_test = fetch_20newsgroups(data_path ,remove=('headers', 'footers', 'quotes'),subset='test')\n",
        "  test_labels = newsgroups_data_test.target\n",
        "  test_texts = newsgroups_data_test.data\n",
        "  return train_texts,train_labels, test_texts, test_labels\n",
        "\n",
        "\n",
        "\n",
        "def get_epoch_frm_model_file(save_path):\n",
        "  return int(save_path[:save_path.index('_')])\n",
        "  # return int(save_path.split('_')[2])\n",
        "\n",
        "def return_last_saved_model(paths):\n",
        "  last_epoch_prev = -1\n",
        "  ret_path = \"\"\n",
        "  for path in paths:\n",
        "    last_epoch = get_epoch_frm_model_file(path)\n",
        "    if(last_epoch >= last_epoch_prev):\n",
        "      last_epoch_prev = last_epoch\n",
        "      ret_path = path\n",
        "  return ret_path\n",
        "\n",
        "\n",
        "def get_embedding_matrix(tokenizer, glove_path, embedding_dim):\n",
        "    \"\"\"\n",
        "    :param path: path to the glove embeddings file\n",
        "    :param tokenizer: tokenizer fitted on the documents\n",
        "    :param vocab_size: vocabulary size \n",
        "    :return: an embedding matrix: a nn.Embeddings\n",
        "    \"\"\"\n",
        "    glove_vectors = {}\n",
        "    glove_file = open(glove_path, 'r')\n",
        "    for line in tqdm(glove_file):\n",
        "        split_line = line.rstrip().split()\n",
        "        word = split_line[0]\n",
        "        if len(split_line) != (embedding_dim + 1) or word not in tokenizer.word_index:\n",
        "            continue\n",
        "        assert (len(split_line) == embedding_dim + 1)\n",
        "        vector = np.array([float(x) for x in split_line[1:]], dtype=\"float32\")\n",
        "        assert len(vector) == embedding_dim\n",
        "        glove_vectors[word] = vector\n",
        "\n",
        "    glove_file.close()\n",
        "\n",
        "    print(\"Number of pre-trained word vectors loaded: \", len(glove_vectors))\n",
        "\n",
        "    # Calculate mean and stdev of embeddings\n",
        "    all_embeddings = np.array(list(glove_vectors.values()))\n",
        "    embeddings_mean = float(np.mean(all_embeddings))\n",
        "    embeddings_stdev = float(np.std(all_embeddings))\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index)\n",
        "    # Randomly initialize an embedding matrix of (vocab_size, embedding_dim) shape\n",
        "    # with a similar distribution as the pretrained embeddings for words in vocab.\n",
        "    embedding_matrix = np.random.normal(embeddings_mean, embeddings_stdev, (vocab_size, embedding_dim))\n",
        "\n",
        "    for i, word in enumerate(tokenizer.word_index):\n",
        "        if word in glove_vectors:\n",
        "            embedding_matrix[i] = glove_vectors[word]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "    # load embedding into memory, skip first line\n",
        "    file = open(filename,'r')\n",
        "    lines = file.readlines()[1:]\n",
        "    file.close()\n",
        "    # create a map of words to vectors\n",
        "    embedding = dict()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        # key is string word, value is numpy array for vector\n",
        "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
        "    return embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab,embed_dim_w):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, embed_dim_w))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = embedding.get(word)\n",
        "    return weight_matrix\n",
        "\n",
        "\n",
        "\n",
        "def get_w2v_matrix(vocab, embed_dim_w):\n",
        "  # load embedding from file\n",
        "  raw_embedding = load_embedding('word2vec_200.txt')\n",
        "  # get vectors in the right order\n",
        "  return(get_weight_matrix(raw_embedding, vocab,embed_dim_w))\n",
        "\n",
        "\n",
        "def prepare_for_word_to_vec(texts):\n",
        "  words_list = []\n",
        "  for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [word for word in tokens if word.isalpha()]\n",
        "    words_list.append(words)\n",
        "  return words_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zXGi21mt7Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in paper\n",
        "vocab_size = None\n",
        "embed_dim = 300 \n",
        "embed_dim_w = 200\n",
        "hidden_dim  = 128\n",
        "batch_size  = 128\n",
        "epochs = 500\n",
        "init_epoch = 0\n",
        "sl = 300\n",
        "vsplit = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as34xea2_7YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_embedding_matrix_path = PATH + 'embedding_matrix.pkl'\n",
        "train_texts,train_labels, test_texts, test_labels = get_data(DATA_PATH)\n",
        "\n",
        "# Should fit_to_texts complete or only train-  https://stackoverflow.com/questions/54891464/is-it-better-to-keras-fit-to-text-on-the-entire-x-data-or-just-the-train-data\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "# fit only on train\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "\n",
        "train_tokenized_seq = tokenize_sequences(train_texts, sl,tokenizer)\n",
        "test_tokenized_seq = tokenize_sequences(test_texts, sl,tokenizer)\n",
        "vocab_size = len(tokenizer.word_index)# +1 for OOV\n",
        "\n",
        "train_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))\n",
        "output_dim = train_labels.shape[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_UnbusW-yrj",
        "colab_type": "code",
        "outputId": "1c04ef53-5a33-4289-a494-f7040be95e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105373"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql_Q4mRiqBMk",
        "colab_type": "text"
      },
      "source": [
        "# W2V model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5QZ03K_qD3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # word2vec mpdel\n",
        "# path = get_tmpfile(PATH+'word2vec_200.txt')\n",
        "# if(glob.glob('word2vec_200.txt')):\n",
        "#   print(\"loading word2vec model\")\n",
        "#   model_w = KeyedVectors.load_word2vec_format(path)\n",
        "# else:\n",
        "#   import nltk\n",
        "#   nltk.download('punkt')\n",
        "#   from nltk.tokenize import word_tokenize\n",
        "#   w_train = prepare_for_word_to_vec(train_texts)\n",
        "#   w_test = prepare_for_word_to_vec(test_texts)\n",
        "#   model_w = Word2Vec(w_train+w_test,size=embed_dim_w,sg=1,iter = 15)\n",
        "\n",
        "\n",
        "# # load weights\n",
        "# if(glob.glob('w2v_weights_200.pkl')):\n",
        "#   print(\"loading w2v weights\")\n",
        "#   w2v_weights = np.load('w2v_weights_200.pkl', allow_pickle=True)\n",
        "#   # vocab_size_w=vocab_size+1\n",
        "\n",
        "# else:\n",
        "#   # get weights\n",
        "#   w2v_weights = get_w2v_matrix(tokenizer.word_index,embed_dim_w)\n",
        "#   # save weights\n",
        "#   pth = PATH+'w2v_weights_200.pkl'\n",
        "#   with open(pth, 'wb+') as f:\n",
        "#       pickle.dump(w2v_weights, f)\n",
        "\n",
        "# vocab_size_w=vocab_size+1\n",
        "\n",
        "\n",
        "# w2v_weights.shape\n",
        "# print(vocab_size_w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQkufQxFqF1R",
        "colab_type": "text"
      },
      "source": [
        "# Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grGp8eLoj1IK",
        "colab_type": "code",
        "outputId": "f0b7d52d-2682-4c45-8fea-5ce0b06c8de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "save_embedding_matrix_path = PATH + 'embedding_matrix_with_split_after_process.pkl'\n",
        "\n",
        "if(glob.glob(save_embedding_matrix_path)):\n",
        "  print('Embedding matrix found. Loading ...')\n",
        "  with open(save_embedding_matrix_path, 'rb') as f:\n",
        "    embedding_matrix = pickle.load(f)\n",
        "\n",
        "  print('Done.')\n",
        "\n",
        "else:\n",
        "  embedding_matrix = get_embedding_matrix(tokenizer, GLOVE_PATH, embed_dim)\n",
        "  with open(save_embedding_matrix_path, 'wb+') as f:\n",
        "    pickle.dump(embedding_matrix, f)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2196017it [02:12, 16562.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of pre-trained word vectors loaded:  56733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKJRdWgJt8MM",
        "colab_type": "code",
        "outputId": "b5569af8-a970-4655-d53e-34cd09af937d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(train_tokenized_seq.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "print(embedding_matrix.shape)\n",
        "print(vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 300)\n",
            "(11314, 20)\n",
            "(105373, 300)\n",
            "105373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzOx9UXS6Zjh",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRAETz0q0CzP",
        "colab_type": "text"
      },
      "source": [
        "## 1) WRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHlUe2k2izJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2495b28-2218-4e1b-95fb-c224a2e53f5c"
      },
      "source": [
        "# architecture \n",
        "inp = Input(shape= (sl,))\n",
        "embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True )(inp)\n",
        "lstm = LSTM(hidden_dim,return_sequences=True,recurrent_dropout=0.5,dropout=0.5)(embed) # o/p 300x128\n",
        "# Note o/p of a rnn cel or lstm cell gives prob of each word - (300,)\n",
        "# conv 1d inp shape - (128,300) -- reshape needed\n",
        "f1 = Flatten()(lstm)\n",
        "r2 = Reshape((hidden_dim,sl))(f1)\n",
        "conv1d = Conv1D(128,1, activation='relu')(r2)\n",
        "gp = GlobalMaxPooling1D()(conv1d)\n",
        "d = Dense(hidden_dim,activation='relu')(gp)\n",
        "out = Dense(output_dim, activation='softmax',kernel_initializer='he_normal', activity_regularizer=l1_l2(0.01,0.01))(d)\n",
        "model_wrnn = Model(input = inp, outputs = out)\n",
        "model_wrnn.summary()\n",
        "\n",
        "# training\n",
        "history = run_model(model_wrnn,train_tokenized_seq,train_labels,epochs,batch_size,vsplit)\n",
        "training_plots(history, 'wrnn.png',model_wrnn)\n",
        "\n",
        "# evaluation\n",
        "print(\"Evaluating...\")\n",
        "evaluate(model_wrnn,test_tokenized_seq, test_labels,batch_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 300, 300)          31611900  \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 300, 128)          219648    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 38400)             0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 128, 300)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 128, 128)          38528     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 20)                2580      \n",
            "=================================================================\n",
            "Total params: 31,889,168\n",
            "Trainable params: 31,889,168\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "START TRAINING\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 11200 samples, validate on 114 samples\n",
            "Epoch 1/500\n",
            "11200/11200 [==============================] - 35s 3ms/step - loss: 4.3084 - categorical_accuracy: 0.0699 - val_loss: 3.9549 - val_categorical_accuracy: 0.1228\n",
            "Epoch 2/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 3.7725 - categorical_accuracy: 0.2047 - val_loss: 3.3491 - val_categorical_accuracy: 0.2719\n",
            "\n",
            "Epoch 00002: val_loss improved from inf to 3.34908, saving model to 02_0.27.h5\n",
            "Epoch 3/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 3.3114 - categorical_accuracy: 0.3665 - val_loss: 3.3896 - val_categorical_accuracy: 0.2807\n",
            "Epoch 4/500\n",
            "11200/11200 [==============================] - 28s 2ms/step - loss: 3.0454 - categorical_accuracy: 0.4916 - val_loss: 3.3209 - val_categorical_accuracy: 0.4211\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.34908 to 3.32094, saving model to 04_0.42.h5\n",
            "Epoch 5/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.8802 - categorical_accuracy: 0.5894 - val_loss: 3.4325 - val_categorical_accuracy: 0.4123\n",
            "Epoch 6/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.7737 - categorical_accuracy: 0.6584 - val_loss: 3.3614 - val_categorical_accuracy: 0.4386\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 3.32094\n",
            "Epoch 7/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.7025 - categorical_accuracy: 0.7140 - val_loss: 3.4704 - val_categorical_accuracy: 0.4386\n",
            "Epoch 8/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.6462 - categorical_accuracy: 0.7498 - val_loss: 3.4411 - val_categorical_accuracy: 0.3947\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 3.32094\n",
            "Epoch 9/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.6030 - categorical_accuracy: 0.7914 - val_loss: 3.3681 - val_categorical_accuracy: 0.4561\n",
            "Epoch 10/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.5729 - categorical_accuracy: 0.8124 - val_loss: 3.2790 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00010: val_loss improved from 3.32094 to 3.27901, saving model to 10_0.50.h5\n",
            "Epoch 11/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.5458 - categorical_accuracy: 0.8327 - val_loss: 3.2423 - val_categorical_accuracy: 0.5000\n",
            "Epoch 12/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.5382 - categorical_accuracy: 0.8388 - val_loss: 3.3034 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.27901\n",
            "Epoch 13/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.5131 - categorical_accuracy: 0.8618 - val_loss: 3.3738 - val_categorical_accuracy: 0.5175\n",
            "Epoch 14/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.5000 - categorical_accuracy: 0.8719 - val_loss: 3.2776 - val_categorical_accuracy: 0.5088\n",
            "\n",
            "Epoch 00014: val_loss improved from 3.27901 to 3.27758, saving model to 14_0.51.h5\n",
            "Epoch 15/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.4920 - categorical_accuracy: 0.8796 - val_loss: 3.2265 - val_categorical_accuracy: 0.5789\n",
            "Epoch 16/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.4782 - categorical_accuracy: 0.8936 - val_loss: 3.3198 - val_categorical_accuracy: 0.4825\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 3.27758\n",
            "Epoch 17/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.4718 - categorical_accuracy: 0.8946 - val_loss: 3.3157 - val_categorical_accuracy: 0.5614\n",
            "Epoch 18/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.4666 - categorical_accuracy: 0.8995 - val_loss: 3.3597 - val_categorical_accuracy: 0.4912\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.27758\n",
            "Epoch 19/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.4539 - categorical_accuracy: 0.9130 - val_loss: 3.3781 - val_categorical_accuracy: 0.5263\n",
            "Epoch 20/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.4482 - categorical_accuracy: 0.9143 - val_loss: 3.3040 - val_categorical_accuracy: 0.5351\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.27758\n",
            "Epoch 21/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.4438 - categorical_accuracy: 0.9204 - val_loss: 3.3816 - val_categorical_accuracy: 0.5877\n",
            "Epoch 22/500\n",
            "11200/11200 [==============================] - 28s 3ms/step - loss: 2.4394 - categorical_accuracy: 0.9251 - val_loss: 3.5228 - val_categorical_accuracy: 0.5439\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.27758\n",
            "Epoch 23/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.4338 - categorical_accuracy: 0.9259 - val_loss: 3.4683 - val_categorical_accuracy: 0.4912\n",
            "Epoch 24/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.4317 - categorical_accuracy: 0.9277 - val_loss: 3.3361 - val_categorical_accuracy: 0.5439\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 3.27758\n",
            "Epoch 25/500\n",
            "11200/11200 [==============================] - 29s 3ms/step - loss: 2.4335 - categorical_accuracy: 0.9288 - val_loss: 3.3538 - val_categorical_accuracy: 0.5439\n",
            "Epoch 26/500\n",
            " 7040/11200 [=================>............] - ETA: 10s - loss: 2.4292 - categorical_accuracy: 0.9344"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQCVLzrhobu",
        "colab_type": "text"
      },
      "source": [
        "### w2v"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asjCZPhL6MnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # W2V \n",
        "\n",
        "\n",
        "# inp = Input(shape= (sl,))\n",
        "# embed = Embedding(input_dim=vocab_size_w, output_dim=embed_dim_w, weights= [w2v_weights], input_length=sl, trainable = True )(inp)\n",
        "# # embed = Embedding(input_dim=vocab_size, output_dim=embed_dim_w,  input_length=sl, trainable = True )(inp)\n",
        "# # embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=sl )(inp)\n",
        "# lstm = LSTM(hidden_dim,return_sequences=True,recurrent_dropout=0.2,dropout=0.2)(embed) # o/p 300x128\n",
        "# # Note o/p of a rnn cel or lstm cell gives prob of each word - (300,)\n",
        "# # conv 1d inp shape - (128,300) -- reshape needed\n",
        "# f1 = Flatten()(lstm)\n",
        "# r2 = Reshape((hidden_dim,sl))(f1)\n",
        "# conv1d = Conv1D(128,1, activation='relu' )(r2)\n",
        "# gp = GlobalMaxPooling1D()(conv1d)\n",
        "# # dense = Dense(1,activation='relu')(conv1d)\n",
        "# # f = Flatten()(gp)\n",
        "# d = Dense(hidden_dim,activation='relu')(gp)\n",
        "# out = Dense(output_dim, activation='softmax', activity_regularizer=l1_l2(0.01,0.01))(d)\n",
        "# model = Model(input = inp, outputs = out)\n",
        "# model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hng23UmrEo5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # bidirectional 0.88\n",
        "\n",
        "\n",
        "# inp = Input(shape= (sl,))\n",
        "# embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True )(inp)\n",
        "# # embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=sl )(inp)\n",
        "# lstm = Bidirectional(LSTM(hidden_dim,return_sequences=True,recurrent_dropout=0.2,dropout=0.2))(embed) # o/p 300x256\n",
        "# # Note o/p of a rnn cel or lstm cell gives prob of each word - (300,)\n",
        "# # conv 1d inp shape - (128,300) -- reshape needed\n",
        "# f1 = Flatten()(lstm)\n",
        "# r2 = Reshape((2*hidden_dim,sl))(f1)\n",
        "# conv1d = Conv1D(hidden_dim,1, activation='relu' )(r2)\n",
        "# # gp = MaxPooling1D(32)(conv1d)\n",
        "# gp = GlobalMaxPooling1D()(conv1d)\n",
        "# # dense = Dense(1,activation='relu')(conv1d)\n",
        "# # f = Flatten()(gp)\n",
        "# d = Dense(hidden_dim,activation='relu')(gp)\n",
        "# out = Dense(output_dim, activation='softmax', activity_regularizer=l1_l2(0.01,0.01))(d)\n",
        "# model = Model(input = inp, outputs = out)\n",
        "# model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GxXaNXuhqbC",
        "colab_type": "text"
      },
      "source": [
        "## 2) Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AJsF2y8YJuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rnn = Sequential([Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True ),\n",
        "                        LSTM(hidden_dim,recurrent_dropout=0.2,dropout=0.2),\n",
        "                        Dense(hidden_dim,activation='relu'),\n",
        "                        Dense(output_dim,activation='softmax')])\n",
        "\n",
        "history_rnn = run_model(model_rnn,train_tokenized_seq,train_labels,epochs,batch_size,vsplit)\n",
        "training_plots(history_rnn, 'simple_rnn.png', model_rnn)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "evaluate(model_rnn,test_tokenized_seq, test_labels,batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r32iako05TDd",
        "colab_type": "text"
      },
      "source": [
        "## 3) BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbYmhnY5Skd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_BiLSTM = Sequential([Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True ),\n",
        "                        Bidirectional(LSTM(hidden_dim,recurrent_dropout=0.2,dropout=0.2)),\n",
        "                        Dense(hidden_dim,activation='relu'),\n",
        "                        Dense(output_dim,activation='softmax')])\n",
        "\n",
        "history_bilstm = run_model(model_BiLSTM,train_tokenized_seq,train_labels,epochs,batch_size,vsplit)\n",
        "training_plots(history_bilstm, 'BiLSTM.png', model_BiLSTM)\n",
        "\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "evaluate(model_BiLSTM,test_tokenized_seq, test_labels,batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}