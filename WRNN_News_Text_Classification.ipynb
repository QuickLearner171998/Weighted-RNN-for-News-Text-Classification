{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WRNN-News_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ql_Q4mRiqBMk",
        "EDQCVLzrhobu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/Weighted-RNN-for-News-Text-Classification/blob/master/WRNN_News_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F5vE8urN7n9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CmxHdbUDQpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% cd \"/content/drive/My Drive/nnfl_project/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V6VsSotwNhy",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8zYf1TVwM5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras.layers import *\n",
        "#Input,Activation, Dense, Embedding, Flatten, LSTM, Conv1D,MaxPooling1D,Dropout,Bidirectional,Reshape,MaxPooling1D,GlobalMaxPooling1D, Dropout,Permute,TimeDistributed,BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential, Model\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.models import load_model\n",
        "from keras import regularizers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "\n",
        "PATH = '/content/drive/My Drive/nnfl_project/'\n",
        "DATA_PATH = '/content/drive/My Drive/nnfl_project/data'\n",
        "GLOVE_PATH = '/content/drive/My Drive/glove.840B.300d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefAWNV2FNtj",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8x0nRb9BJUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(model,y_texts, y_labels, batch_size):\n",
        "  loss, acc = model.evaluate(y_texts,y_labels,batch_size=batch_size,verbose=1)\n",
        "  # print(\"Accuracy of the model\",acc)\n",
        "  y_pred = model.predict(y_texts, batch_size=batch_size)\n",
        "  y_pred_max_ind = np.argmax(y_pred,axis=1)\n",
        "  y_true_max_ind = np.argmax(y_labels,axis=1)\n",
        "  cf = (classification_report(y_true_max_ind,y_pred_max_ind))\n",
        "\n",
        "  print(\"\\n Model Accuracy : \",acc)\n",
        "  print(\"\\nClassification Report\\n \",cf)\n",
        "\n",
        "\n",
        "\n",
        "# model_op_folder = 'new_conv1d_GMP_split'\n",
        "def get_callbacks():\n",
        "\n",
        "  chk_pth = (\"{epoch:02d}_{val_categorical_accuracy:.2f}.h5\")\n",
        "  # early stopping\n",
        "  earlyStop = EarlyStopping(monitor='val_loss', mode='auto', verbose=1,patience=16)\n",
        "  checkpoint = ModelCheckpoint(chk_pth,monitor='val_loss', verbose=1, save_weights_only=True, save_best_only='True',mode = 'auto', period=2)\n",
        "\n",
        "  # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001, verbose = 1)\n",
        "  cb = [checkpoint,earlyStop]\n",
        "  return cb\n",
        "\n",
        "\n",
        "\n",
        "def run_model(model,train_tokenized_seq,train_labels,epochs,batch_size,vsplit,init_epoch=0):\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01),metrics=['categorical_accuracy'])\n",
        "\n",
        "  # check if a saved_model exists\n",
        "  if(glob.glob('*.h5')):\n",
        "    print(\"loading model\")\n",
        "    load_path = return_last_saved_model(glob.glob('*.h5'))\n",
        "    # model = load_model(load_path)\n",
        "    model.load_weights(load_path)\n",
        "    init_epoch = get_epoch_frm_model_file(load_path)\n",
        "\n",
        "\n",
        "  print(\"START TRAINING\")\n",
        "\n",
        "  # history = model.fit(train_tokenized_seq,train_labels,epochs = init_epoch+epochs, batch_size=batch_size, validation_data=(test_tokenized_seq, test_labels), callbacks=[checkpoint,earlyStop], initial_epoch=init_epoch)\n",
        "  history = model.fit(train_tokenized_seq,train_labels,epochs = init_epoch+epochs, batch_size=batch_size, validation_split=vsplit, callbacks=get_callbacks(), initial_epoch=init_epoch)\n",
        "\n",
        "\n",
        "  print(\"Training fininshed\")\n",
        "  # save_path = \"{}_epochs-model.h5\".format(init_epoch+epochs)\n",
        "  # model.save(save_path)\n",
        "  return history\n",
        "\n",
        "\n",
        "\n",
        "# plot model\n",
        "\n",
        "def training_plots(history, model_name,model):  \n",
        "  plt.plot(history.history[\"categorical_accuracy\"])\n",
        "  plt.plot(history.history[\"val_categorical_accuracy\"])\n",
        "  plt.title('model acc')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history[\"loss\"])\n",
        "  plt.plot(history.history[\"val_loss\"])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plot_model(model,to_file=model_name,show_shapes=True)\n",
        "\n",
        "\n",
        "def tokenize_sequences(texts, sl,tokenizer):\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  sequences = pad_sequences(sequences, maxlen=sl)\n",
        "\n",
        "  return sequences\n",
        "\n",
        "def get_data(data_path):\n",
        "  newsgroups_data_train = fetch_20newsgroups(data_path,subset='train')\n",
        "  train_labels = newsgroups_data_train.target\n",
        "  train_texts = newsgroups_data_train.data\n",
        "\n",
        "  newsgroups_data_test = fetch_20newsgroups(data_path,subset='test')\n",
        "  test_labels = newsgroups_data_test.target\n",
        "  test_texts = newsgroups_data_test.data\n",
        "  return train_texts,train_labels, test_texts, test_labels\n",
        "\n",
        "\n",
        "\n",
        "def get_epoch_frm_model_file(save_path):\n",
        "  return int(save_path[:save_path.index('_')])\n",
        "  # return int(save_path.split('_')[2])\n",
        "\n",
        "def return_last_saved_model(paths):\n",
        "  last_epoch_prev = -1\n",
        "  ret_path = \"\"\n",
        "  for path in paths:\n",
        "    last_epoch = get_epoch_frm_model_file(path)\n",
        "    if(last_epoch >= last_epoch_prev):\n",
        "      last_epoch_prev = last_epoch\n",
        "      ret_path = path\n",
        "  return ret_path\n",
        "\n",
        "\n",
        "def get_embedding_matrix(tokenizer, glove_path, embedding_dim):\n",
        "    \"\"\"\n",
        "    :param path: path to the glove embeddings file\n",
        "    :param tokenizer: tokenizer fitted on the documents\n",
        "    :param vocab_size: vocabulary size \n",
        "    :return: an embedding matrix: a nn.Embeddings\n",
        "    \"\"\"\n",
        "    glove_vectors = {}\n",
        "    glove_file = open(glove_path, 'r')\n",
        "    for line in tqdm(glove_file):\n",
        "        split_line = line.rstrip().split()\n",
        "        word = split_line[0]\n",
        "        if len(split_line) != (embedding_dim + 1) or word not in tokenizer.word_index:\n",
        "            continue\n",
        "        assert (len(split_line) == embedding_dim + 1)\n",
        "        vector = np.array([float(x) for x in split_line[1:]], dtype=\"float32\")\n",
        "        assert len(vector) == embedding_dim\n",
        "        glove_vectors[word] = vector\n",
        "\n",
        "    glove_file.close()\n",
        "\n",
        "    print(\"Number of pre-trained word vectors loaded: \", len(glove_vectors))\n",
        "\n",
        "    # Calculate mean and stdev of embeddings\n",
        "    all_embeddings = np.array(list(glove_vectors.values()))\n",
        "    embeddings_mean = float(np.mean(all_embeddings))\n",
        "    embeddings_stdev = float(np.std(all_embeddings))\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index)\n",
        "    # Randomly initialize an embedding matrix of (vocab_size, embedding_dim) shape\n",
        "    # with a similar distribution as the pretrained embeddings for words in vocab.\n",
        "    embedding_matrix = np.random.normal(embeddings_mean, embeddings_stdev, (vocab_size, embedding_dim))\n",
        "\n",
        "    for i, word in enumerate(tokenizer.word_index):\n",
        "        if word in glove_vectors:\n",
        "            embedding_matrix[i] = glove_vectors[word]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "    # load embedding into memory, skip first line\n",
        "    file = open(filename,'r')\n",
        "    lines = file.readlines()[1:]\n",
        "    file.close()\n",
        "    # create a map of words to vectors\n",
        "    embedding = dict()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        # key is string word, value is numpy array for vector\n",
        "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
        "    return embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab,embed_dim_w):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, embed_dim_w))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = embedding.get(word)\n",
        "    return weight_matrix\n",
        "\n",
        "\n",
        "\n",
        "def get_w2v_matrix(vocab, embed_dim_w):\n",
        "  # load embedding from file\n",
        "  raw_embedding = load_embedding('word2vec_200.txt')\n",
        "  # get vectors in the right order\n",
        "  return(get_weight_matrix(raw_embedding, vocab,embed_dim_w))\n",
        "\n",
        "\n",
        "def prepare_for_word_to_vec(texts):\n",
        "  words_list = []\n",
        "  for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [word for word in tokens if word.isalpha()]\n",
        "    words_list.append(words)\n",
        "  return words_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QvTnP_RFR3m",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zXGi21mt7Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in paper\n",
        "vocab_size = None\n",
        "embed_dim = 300 \n",
        "embed_dim_w = 200\n",
        "hidden_dim  = 128\n",
        "batch_size  = 128\n",
        "epochs = 500\n",
        "init_epoch = 0\n",
        "sl = 300\n",
        "vsplit = 0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSlUU4X_FVeb",
        "colab_type": "text"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as34xea2_7YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_embedding_matrix_path = PATH + 'embedding_matrix.pkl'\n",
        "train_texts,train_labels, test_texts, test_labels = get_data(DATA_PATH)\n",
        "\n",
        "# Should fit_to_texts complete or only train-  https://stackoverflow.com/questions/54891464/is-it-better-to-keras-fit-to-text-on-the-entire-x-data-or-just-the-train-data\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "# fit only on train\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "\n",
        "train_tokenized_seq = tokenize_sequences(train_texts, sl,tokenizer)\n",
        "test_tokenized_seq = tokenize_sequences(test_texts, sl,tokenizer)\n",
        "vocab_size = len(tokenizer.word_index)# +1 for OOV\n",
        "\n",
        "train_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))\n",
        "output_dim = train_labels.shape[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_UnbusW-yrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql_Q4mRiqBMk",
        "colab_type": "text"
      },
      "source": [
        "# W2V model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5QZ03K_qD3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # word2vec mpdel\n",
        "# path = get_tmpfile(PATH+'word2vec_200.txt')\n",
        "# if(glob.glob('word2vec_200.txt')):\n",
        "#   print(\"loading word2vec model\")\n",
        "#   model_w = KeyedVectors.load_word2vec_format(path)\n",
        "# else:\n",
        "#   import nltk\n",
        "#   nltk.download('punkt')\n",
        "#   from nltk.tokenize import word_tokenize\n",
        "#   w_train = prepare_for_word_to_vec(train_texts)\n",
        "#   w_test = prepare_for_word_to_vec(test_texts)\n",
        "#   model_w = Word2Vec(w_train+w_test,size=embed_dim_w,sg=1,iter = 15)\n",
        "\n",
        "\n",
        "# # load weights\n",
        "# if(glob.glob('w2v_weights_200.pkl')):\n",
        "#   print(\"loading w2v weights\")\n",
        "#   w2v_weights = np.load('w2v_weights_200.pkl', allow_pickle=True)\n",
        "#   # vocab_size_w=vocab_size+1\n",
        "\n",
        "# else:\n",
        "#   # get weights\n",
        "#   w2v_weights = get_w2v_matrix(tokenizer.word_index,embed_dim_w)\n",
        "#   # save weights\n",
        "#   pth = PATH+'w2v_weights_200.pkl'\n",
        "#   with open(pth, 'wb+') as f:\n",
        "#       pickle.dump(w2v_weights, f)\n",
        "\n",
        "# vocab_size_w=vocab_size+1\n",
        "\n",
        "\n",
        "# w2v_weights.shape\n",
        "# print(vocab_size_w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQkufQxFqF1R",
        "colab_type": "text"
      },
      "source": [
        "# Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grGp8eLoj1IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_embedding_matrix_path = PATH + 'embedding_matrix_with_split.pkl'\n",
        "\n",
        "if(glob.glob(save_embedding_matrix_path)):\n",
        "  print('Embedding matrix found. Loading ...')\n",
        "  with open(save_embedding_matrix_path, 'rb') as f:\n",
        "    embedding_matrix = pickle.load(f)\n",
        "\n",
        "  print('Done.')\n",
        "\n",
        "else:\n",
        "  embedding_matrix = get_embedding_matrix(tokenizer, GLOVE_PATH, embed_dim)\n",
        "  with open(save_embedding_matrix_path, 'wb+') as f:\n",
        "    pickle.dump(embedding_matrix, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKJRdWgJt8MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_tokenized_seq.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "print(embedding_matrix.shape)\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzOx9UXS6Zjh",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRAETz0q0CzP",
        "colab_type": "text"
      },
      "source": [
        "## 1) WRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHlUe2k2izJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# architecture \n",
        "inp = Input(shape= (sl,))\n",
        "embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True )(inp)\n",
        "lstm = LSTM(hidden_dim,return_sequences=True,recurrent_dropout=0.5,dropout=0.5)(embed) # o/p 300x128\n",
        "# Note o/p of a rnn cel or lstm cell gives prob of each word - (300,)\n",
        "# conv 1d inp shape - (128,300) -- reshape needed\n",
        "f1 = Flatten()(lstm)\n",
        "r2 = Reshape((hidden_dim,sl))(f1)\n",
        "conv1d = Conv1D(128,1, activation='relu')(r2)\n",
        "gp = GlobalMaxPooling1D()(conv1d)\n",
        "d = Dense(hidden_dim,activation='relu')(gp)\n",
        "out = Dense(output_dim, activation='softmax',kernel_initializer='he_normal', activity_regularizer=l1_l2(0.01,0.01))(d)\n",
        "model_wrnn = Model(input = inp, outputs = out)\n",
        "model_wrnn.summary()\n",
        "\n",
        "# training\n",
        "history = run_model(model_wrnn,train_tokenized_seq,train_labels,epochs,batch_size,vsplit)\n",
        "training_plots(history, 'wrnn.png',model_wrnn)\n",
        "\n",
        "# evaluation\n",
        "print(\"Evaluating...\")\n",
        "evaluate(model_wrnn,test_tokenized_seq, test_labels,batch_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQCVLzrhobu",
        "colab_type": "text"
      },
      "source": [
        "### w2v"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asjCZPhL6MnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # W2V \n",
        "\n",
        "\n",
        "# inp = Input(shape= (sl,))\n",
        "# embed = Embedding(input_dim=vocab_size_w, output_dim=embed_dim_w, weights= [w2v_weights], input_length=sl, trainable = True )(inp)\n",
        "# # embed = Embedding(input_dim=vocab_size, output_dim=embed_dim_w,  input_length=sl, trainable = True )(inp)\n",
        "# # embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=sl )(inp)\n",
        "# lstm = LSTM(hidden_dim,return_sequences=True,recurrent_dropout=0.2,dropout=0.2)(embed) # o/p 300x128\n",
        "# # Note o/p of a rnn cel or lstm cell gives prob of each word - (300,)\n",
        "# # conv 1d inp shape - (128,300) -- reshape needed\n",
        "# f1 = Flatten()(lstm)\n",
        "# r2 = Reshape((hidden_dim,sl))(f1)\n",
        "# conv1d = Conv1D(128,1, activation='relu' )(r2)\n",
        "# gp = GlobalMaxPooling1D()(conv1d)\n",
        "# # dense = Dense(1,activation='relu')(conv1d)\n",
        "# # f = Flatten()(gp)\n",
        "# d = Dense(hidden_dim,activation='relu')(gp)\n",
        "# out = Dense(output_dim, activation='softmax', activity_regularizer=l1_l2(0.01,0.01))(d)\n",
        "# model = Model(input = inp, outputs = out)\n",
        "# model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hng23UmrEo5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # bidirectional 0.88\n",
        "\n",
        "\n",
        "# inp = Input(shape= (sl,))\n",
        "# embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True )(inp)\n",
        "# # embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=sl )(inp)\n",
        "# lstm = Bidirectional(LSTM(hidden_dim,return_sequences=True,recurrent_dropout=0.2,dropout=0.2))(embed) # o/p 300x256\n",
        "# # Note o/p of a rnn cel or lstm cell gives prob of each word - (300,)\n",
        "# # conv 1d inp shape - (128,300) -- reshape needed\n",
        "# f1 = Flatten()(lstm)\n",
        "# r2 = Reshape((2*hidden_dim,sl))(f1)\n",
        "# conv1d = Conv1D(hidden_dim,1, activation='relu' )(r2)\n",
        "# # gp = MaxPooling1D(32)(conv1d)\n",
        "# gp = GlobalMaxPooling1D()(conv1d)\n",
        "# # dense = Dense(1,activation='relu')(conv1d)\n",
        "# # f = Flatten()(gp)\n",
        "# d = Dense(hidden_dim,activation='relu')(gp)\n",
        "# out = Dense(output_dim, activation='softmax', activity_regularizer=l1_l2(0.01,0.01))(d)\n",
        "# model = Model(input = inp, outputs = out)\n",
        "# model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GxXaNXuhqbC",
        "colab_type": "text"
      },
      "source": [
        "## 2) Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AJsF2y8YJuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rnn = Sequential([Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True ),\n",
        "                        LSTM(hidden_dim,recurrent_dropout=0.2,dropout=0.2),\n",
        "                        Dense(hidden_dim,activation='relu'),\n",
        "                        Dense(output_dim,activation='softmax')])\n",
        "\n",
        "history_rnn = run_model(model_rnn,train_tokenized_seq,train_labels,epochs,batch_size,vsplit)\n",
        "training_plots(history_rnn, 'simple_rnn.png', model_rnn)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "evaluate(model_rnn,test_tokenized_seq, test_labels,batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r32iako05TDd",
        "colab_type": "text"
      },
      "source": [
        "## 3) BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbYmhnY5Skd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_BiLSTM = Sequential([Embedding(input_dim=vocab_size, output_dim=embed_dim, weights= [embedding_matrix], input_length=sl, trainable = True ),\n",
        "                        Bidirectional(LSTM(hidden_dim,recurrent_dropout=0.2,dropout=0.2)),\n",
        "                        Dense(hidden_dim,activation='relu'),\n",
        "                        Dense(output_dim,activation='softmax')])\n",
        "\n",
        "history_bilstm = run_model(model_BiLSTM,train_tokenized_seq,train_labels,epochs,batch_size,vsplit)\n",
        "training_plots(history_bilstm, 'BiLSTM.png', model_BiLSTM)\n",
        "\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "evaluate(model_BiLSTM,test_tokenized_seq, test_labels,batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}